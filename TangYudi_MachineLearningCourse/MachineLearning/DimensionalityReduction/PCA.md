# PCA(主成分分析)

Principal Component Analysis

用途：降维中最常用的一种手段，无监督学习

目标：提取最有价值的信息（基于方差，使得方差最大）

问题：降维后的数据的意义？（无法确定，对于保密数据进行PCA，用于公开进行测试）

向量的表示及基变换
内积：

$(a_{1},a_{2},...,a_{n})^{T} \cdot (b_{1},b_{2},...,b_{n})^{T}=a_{1}b_{1}+a_{2}b_{2}+...+a_{n}b_{n}$

解释：

$A \cdot B = |A||B|cos(a)$

设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度。

向量的表示及基变换

向量可以表示为（3，2）
实际上表示线性组合为：
$x(1,0)^{T}+y(0,1)^{T}$

基：（1，0）和（0，1）叫做二维空间的一组基

基变换

基是正交的，即内积为零，或直观说相互垂直

要求：线性无关

基变换：

变换：数据与一个基做内积运算，结果作为第一个新的坐标分量，然后与第二个基做内积运算，结果作为第二个新坐标的分量。

数据（3，2）映射到基中坐标

# 缺少公式


协方差矩阵

方向：如何选择这个方向（或者说基）才能尽量保留最多的原始信息？直观的看法是：希望投影后的投影值尽可能分散

方差：

$Var(a)=\frac{1}{m} \sum_{i=1}^{m} (a_{i}-\mu)^2$

寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。


协方差（假设均值为零，会做0中心化）：$Cov(a,b)=\frac{1}{m} \sum_{i=1}^{m} a_{i}b_{i}$

如果单纯只选择方差最大的方向，后续方面会和方差最大的方向接近重合。

解决方案：

为了让两个字段尽可能表示更多的原始信息，我们不希望他们之间存在线性相关性。

协方差：可以用两个字段的协方差性表示其相关性

当协方差为0时，表示两个字段完全独立。为了让协方差为0，选择第二个基时，只能在与第一个基正交的方向上选择。因此最后选择的两个方向正交。

优化目标：

将一组N维向量降维为K维，目标是选择K个单位正交基，使原始数据转换到这组基上后，各字段两两间协方差为0，字段的方差则尽可能大。

协方差矩阵：

$\begin{pmatrix} a_{1} & a_{1} & ... & a_{m} \\ b_{1} & b_{2} & ... & b_{m} \end{pmatrix}$

$\frac{1}{m} XX^{T} = \begin{pmatrix} \frac{1}{m} \sum_{i=1}^{m} a_{i}^{2} & \frac{1}{m} \sum_{i=1}^{m} a_{i}b_{i} \\ \frac{1}{m} \sum_{i=1}^{m} a_{i}b_{i} & \frac{1}{m} \sum_{i=1}^{m} b_{i}^{2} \end{pmatrix}$

矩阵对角线上的两个元素分别是两个字段的方差，而其他元素是a和b的协方差。

优化目标：

协方差矩阵对角化：即除对角线外的其他元素化为0，并且在对角线上将元素按照大小从上到下排列。

实对称矩阵：一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量。

$E = (e1 e2 ... e3)$

实对称矩阵可进行对角化：

$E^{T}CE=\Lambda=\begin{pmatrix} \lambda_{1}& & & \\ & \lambda_{2} & & \\ & & \lambda_{3} & \\ & & & \lambda_{m} \end{pmatrix}$

根据特征值的从大到小，将特征向量从上到下排列，则用前K行组成的矩阵乘以原始矩阵X,就得到了我们需要的降维后的数据矩阵Y,特征值表征特征的重要性，越大越重要。
