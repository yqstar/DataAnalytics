# 机器学习

## ToList



## 线性回归

假设线性回归是个黑盒子，那按照程序员的思维来说，这个黑盒子就是个函数，然后呢，我们只要往这个函数传一些参数作为输入，就能得到一个结果作为输出。那回归是什么意思呢？其实说白了，就是这个黑盒子输出的结果是个连续的值。如果输出不是个连续值而是个离散值那就叫分类。那什么叫做连续值呢？非常简单，举个栗子：比如我告诉你我这里有间房子，这间房子有40平，在地铁口，然后你来猜一猜我的房子总共值多少钱？这就是连续值，因为房子可能值80万，也可能值80.2万，也可能值80.111万。再比如，我告诉你我有间房子，120平，在地铁口，总共值180万，然后你来猜猜我这间房子会有几个卧室？那这就是离散值了。因为卧室的个数只可能是1， 2， 3，4，充其量到5个封顶了，而且卧室个数也不可能是什么1.1， 2.9个。所以呢，对于ML萌新来说，你只要知道我要完成的任务是预测一个连续值的话，那这个任务就是回归。是离散值的话就是分类。（PS:目前只讨论监督学习）


### 一个例子
数据：工资和年龄（两个特征）
目标：预测银行会贷款多少钱（标签）
考虑：工资和年龄都会影响最终的银行贷款的结果，那么他们各自有多大的影响呢？（参数）


### 通俗解释

X1，X2就是我们的两个特征（年龄，工资），Y是银行最终会借给我们多少钱？

找到一条最合适的线（想像成一个高维）来拟合我们的数据点。


### 数学来了

线性回归模型：

假设theta1是年龄的参数，theta2是工资的参数

拟合的平面：h_theta(x) = theta0 + theta1*x1 + theta2*x2

线性回归误差：

真实值和预测值之间肯定是要存在差异的（用epsilon表示）

对于每个样本:y^(i) = theta

## 决策树

### 树模型

决策树：从根节点开始一步步走到叶子节点（决策），所有的数据最终都会落到叶子节点，既可以做分类也可以做回归。

树的组成：

根节点：第一个选择点

非叶子节点与分支：中间过程

叶子节点：最终的决策结果

节点：

增加节点相当于在数据中切一刀

节点越多越好吗？

决策树的训练与测试

训练阶段：从给定的训练集构造出来一棵树（从根节点开始选择特征，如何进行特征切分）

测试阶段：根据构造出来的树模型从上到下走一遍就好了

一旦构造好了决策树，那么分类或者预测任务就很简单了，只需要走一遍就可以了。那么如何构造一棵树呢？

决策树如何切分特征？

问题：根节点的选择该用那个特征呢？接下来呢？如何切分呢？

想象一下：我们的目标应该是根节点就像一个老大似的能更好地切分数据（分类的效果更好），根节点下面的节点自然是二当家了。

目标：通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找出来最好的那个当成根节点，一次类推。

衡量标准：熵

熵是表示随机变量不确定性的度量（解释：说白了就是物体内部的混乱程度，比如杂货市场里面什么都有，那肯定混乱呀，专卖店里面只卖一个牌子的那就稳定多了）,不确定性越大，得到的熵值也就越大。

公式：H(X)=-sum(pi*logpi),i=1,2,3...

一个例子：

A=[1,1,1,1,1,3,3]

B=[1,2,3,4,5,6,7,8]

显然A集合的熵值要低，因为A中只有两种类别，相对稳定一些，而B中类别太多了，熵值就会大很多。（在分类中我们希望通过节点分支后数据类别的熵值大还是小呢？)

如何决策一个节点的选择呢？

信息增益：表示特征X使得类Y的不确定性减少的程度（分类后的专一性，希望分类后的结果是同类在一起）。

决策树剪枝策略

为什么要剪枝：决策树过拟合风险很大，理论上可以完全分的开数据（想象一下，如果树足够庞大，每个叶子节点不就一个数据了吗？）

剪枝策略：预剪枝，后剪枝

预剪枝：边建立决策树边进行剪枝的操作（更实用）。限制深度，叶子节点个数，叶子节点样本数，信息增益量。

后剪枝：当建立完决策树后来进行剪枝操作。通过一定的衡量标准。（叶子节点越多，损失越大）

# 集成算法（Ensemble learning）

目的：让机器学习效果更好，单个不行，群殴走起

Bagging：训练多个分类器取平均 f(x)=1/M sum(f_m(x))

--全称：bootstrap aggregation（说白了就是并行训练一堆分类器）

--最典型的代表就是随机森林
--随机：数据采样随机，特征选择随机
--森林：多个决策树并行放在一起

随机的原因：要保证泛化能力，如果树都一样，那就没有意义了。

随机森林优势：
能够处理高纬度的数据，并且不用做特征选择。
训练结束后，能够给出哪些feature比较重要。
容易做成并行化方法，速度比较快。
可以进行可视化展示，便于分析。

理论上越多的树效果越好，但实际上基本超过一定数量就差不多效果上下波动。


Boosting:从弱学习器开始加强，通过加权来进行训练。

典型代表：AdaBoost，Xgboost

AdaBoost会根据前一次的分类效果调整数据权重

解释：如果某一个数据在这次分错了，那么在下一次我就会给他更大的权重。

最终的结果：每个分类器根据自身的准确性来确定各自的权重，再合体。


Stacking:聚合多个分类或回归模型（可以分阶段来做）

堆叠：很暴力，拿来一堆直接上（各种分类器都来了）

可以堆叠各种各样的分类器（KNN，SVM，RF等）

分阶段：第一阶段的得出各自结果，第二阶段使用第一阶段的结果进行计算。

为了结果，不择手段。

## 贝叶斯概率算法

贝叶斯算法为解决逆概问题。

贝叶斯要解决的问题：

正向概率：假设袋子里有N个黑球，M个白球，摸出黑球的概率有多大？

逆向概率：如果我们事先并不知道袋子里的黑白秋比例，而是闭着眼摸出一个或几个球，观察这些取出来的球的颜色，我们可以对袋子里的黑白秋的比例做出什么样的推测？

Why 贝叶斯？

现实世界本身就是不确定的，人类的观察能力是有局限性的。

我们日常所观察到的只是事物表面上的结果，因此我们需要提供一个猜测。

例子来了

一个学校男生60%，女生40%，男生总是穿长裤，女生一般穿长裤，一般穿裙子。

正向概率：随机选取一个学生，Ta穿长裤的概率和穿裙子的概率是多大？

逆向概率：迎面走来一个穿长裤的学生，只能看到穿长裤，无法确定性别，你能推断Ta是女生的概率吗？

假设学校里总人数为U个

穿长裤的男生：U*P(BOY)*P(Pants|BOY)

P(BOY)是男生的概率=60%

P(Pants|BOY)是条件概率，即在BOY这个条件下穿长裤的概率是多大，这里是100%，因为所有男生都穿长裤

穿长裤的女生：U*P(GIRL)*P(Pants|GIRL)

求解：穿长裤的人里面有多少女生

穿长裤的总数：U*P(BOY)*P(Pants|BOY)+U*P(GIRL)*P(Pants|GIRL)

P(GIRL|Pants)=U*P(GIRL)*P(Pants|GIRL)/穿长裤的总数=U*P(GIRL)*P(Pants|GIRL)/[U*P(BOY)*P(Pants|BOY)+U*P(GIRL)*P(Pants|GIRL)]

化简：

分母其实就是：P(Pants),分子其实就是P(Pants,Girl)

贝叶斯公式：

P(A|B)=P(B|A)*P(A)/P(B)

拼写矫正实例：

问题是：我们看到用户输入了一个不在字典中的单词，我们需要去猜测“这个家伙到底真正想输入的单词是什么呢？”

P(我们猜测他想输入的单词|他实际输入的单词)

用户实际输入的单词记为D(D代表Data，即观察数据)

猜测1：P(h1|D),猜测2：P(h2|D),猜测3：P(h3|D),猜测4：P(h4|D)...统一为：P(h|D)

P(h|D) = P(h) * P(D|h)/P(D)

对于不同的具体猜测h1、h2、。。。P(D)都是一样的，所以在比较P(h1|D)和P(h2|D)的时候我们可以忽略这个常数

P(h|D) 正比于 P(h) * P(D|h)

对于给定的观测数据，一个猜测是好是坏，取决于这个猜测本身独立的可能性大小（先验概率，prior）和这个猜测生成我们观测到的数据的可能性大小。

贝叶斯方法计算：P(h)*P(D|h),P(h)是特定猜测的先验概率

比如用户输入tlp,到底是top还是tip呢？这个时候，当最大似然不能做出决定性判断时，先验概率就可以插手进来给出指示：“既然你无法决定，那么我告诉你，一般来说top出现的程度要高许多，所以更可能他想打top”

模型比较理论

最大似然：最符合观察数据的即P(D|h)最大的最有优势

奥卡姆剃刀：P(h)较大的模型有较大的优势

投一个硬币，观察到的是正面，根据最大似然估计的精神，我们应该猜测这枚硬币投掷出正的概率是1，因为这个才是最大化P(D|h)的那个猜测。

奥卡姆剃刀：如果平面上有N个点，近似构成一条直线，可以用多阶多项式进行拟合？越是高阶的多项式越是不常见。

例子来了：

垃圾邮件过滤实例：

问题：给定一份邮件，判断是否是属于垃圾邮件？

D来表示这份邮件，注意D由N个单词组成。h+表示垃圾邮件，h-表示正常邮件。

P(h+|D) = P(h+) * P(D|h+)/P(D)

P(h-|D) = P(h-) * P(D|h-)/P(D)

先验概率：P(h+)和P(h-)这两个先验概率是很容易求出来的，只需要计算一个邮件库里面的垃圾邮件和正常邮件的比例就行了。

D 里面含有 N 个单词 d1, d2, d3，P(D|h+) = P(d1,d2,..,dn|h+)
P(d1,d2,..,dn|h+) 就是说在垃圾邮件当中出现跟我们目前这封邮件一模一样的一封邮件的概率是多大！

P(d1,d2,..,dn|h+)  扩展为： P(d1|h+) * P(d2|d1, h+) * P(d3|d2,d1, h+) * ..  

P(d1|h+) * P(d2|d1, h+) * P(d3|d2,d1, h+) * ..  
假设 di 与 di-1 是完全条件无关的（朴素贝叶斯假设特征之间是独立，互不影响）
简化为 P(d1|h+) * P(d2|h+) * P(d3|h+) * .. 

对于P(d1|h+) * P(d2|h+) * P(d3|h+) * ..只要统计 di 这个单词在垃圾邮件中出现的频率即可
