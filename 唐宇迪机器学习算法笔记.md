# 机器学习

## ToList



## 线性回归

假设线性回归是个黑盒子，那按照程序员的思维来说，这个黑盒子就是个函数，然后呢，我们只要往这个函数传一些参数作为输入，就能得到一个结果作为输出。那回归是什么意思呢？其实说白了，就是这个黑盒子输出的结果是个连续的值。如果输出不是个连续值而是个离散值那就叫分类。那什么叫做连续值呢？非常简单，举个栗子：比如我告诉你我这里有间房子，这间房子有40平，在地铁口，然后你来猜一猜我的房子总共值多少钱？这就是连续值，因为房子可能值80万，也可能值80.2万，也可能值80.111万。再比如，我告诉你我有间房子，120平，在地铁口，总共值180万，然后你来猜猜我这间房子会有几个卧室？那这就是离散值了。因为卧室的个数只可能是1， 2， 3，4，充其量到5个封顶了，而且卧室个数也不可能是什么1.1， 2.9个。所以呢，对于ML萌新来说，你只要知道我要完成的任务是预测一个连续值的话，那这个任务就是回归。是离散值的话就是分类。（PS:目前只讨论监督学习）


### 一个例子
数据：工资和年龄（两个特征）
目标：预测银行会贷款多少钱（标签）
考虑：工资和年龄都会影响最终的银行贷款的结果，那么他们各自有多大的影响呢？（参数）


### 通俗解释

X1，X2就是我们的两个特征（年龄，工资），Y是银行最终会借给我们多少钱？

找到一条最合适的线（想像成一个高维）来拟合我们的数据点。


### 数学来了

线性回归模型：

假设theta1是年龄的参数，theta2是工资的参数

拟合的平面：h_theta(x) = theta0 + theta1*x1 + theta2*x2

线性回归误差：

真实值和预测值之间肯定是要存在差异的（用epsilon表示）

对于每个样本:y^(i) = theta

## 决策树

### 树模型

决策树：从根节点开始一步步走到叶子节点（决策），所有的数据最终都会落到叶子节点，既可以做分类也可以做回归。

树的组成：

根节点：第一个选择点

非叶子节点与分支：中间过程

叶子节点：最终的决策结果

节点：

增加节点相当于在数据中切一刀

节点越多越好吗？

决策树的训练与测试

训练阶段：从给定的训练集构造出来一棵树（从根节点开始选择特征，如何进行特征切分）

测试阶段：根据构造出来的树模型从上到下走一遍就好了

一旦构造好了决策树，那么分类或者预测任务就很简单了，只需要走一遍就可以了。那么如何构造一棵树呢？

决策树如何切分特征？

问题：根节点的选择该用那个特征呢？接下来呢？如何切分呢？

想象一下：我们的目标应该是根节点就像一个老大似的能更好地切分数据（分类的效果更好），根节点下面的节点自然是二当家了。

目标：通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找出来最好的那个当成根节点，一次类推。

衡量标准：熵

熵是表示随机变量不确定性的度量（解释：说白了就是物体内部的混乱程度，比如杂货市场里面什么都有，那肯定混乱呀，专卖店里面只卖一个牌子的那就稳定多了）,不确定性越大，得到的熵值也就越大。

公式：H(X)=-sum(pi*logpi),i=1,2,3...

一个例子：

A=[1,1,1,1,1,3,3]

B=[1,2,3,4,5,6,7,8]

显然A集合的熵值要低，因为A中只有两种类别，相对稳定一些，而B中类别太多了，熵值就会大很多。（在分类中我们希望通过节点分支后数据类别的熵值大还是小呢？)

如何决策一个节点的选择呢？

信息增益：表示特征X使得类Y的不确定性减少的程度（分类后的专一性，希望分类后的结果是同类在一起）。









