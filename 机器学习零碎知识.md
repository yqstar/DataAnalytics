# 机器学习零碎知识

## 向量的内积和外积

* 内积（点乘）

a·b=a1b1+a2b2+……+anbn。

其中：向量a = [a1, a2,…, an] 和 b = [b1, b2,…, bn]

* 外积（叉乘）

|a×b| = |a|·|b|·sin<a,b>

其中：方向根据右手法则确定，就是手掌立在a、b所在平面的向量a上，掌心由a转向b的过程中，大拇指的方向就是外积的方向。

## 格拉姆矩阵(Gram Matrix)

## KD-Tree原理、构建及查询

## 二叉查找树（BST)、平衡二叉树(AVL树)

## Numpy中的矩阵计算

## 概率分布和概率密度函数

## 贝叶斯公式

## 缺失值处理
缺失值补充 造成属性值缺失的原因有很多，比如信息暂时无法获取，信息被遗漏（有机械原因也有人为原因），有些对象的某个或者某些属性是不可用的，有些信息被认为是不重要的，获取这些信息的代价太大，系统实时性能要求较高；此外对缺失值的处理一定要具体问题具体分析，因为缺失值并不意味着数据缺失，缺失本身也是包含信息的，所以需要根据不同的场景下的缺失值进行合理填充。 目前缺失值的类型，含有缺失值的变量被称为不完全变量，而不含有缺失值的变量被称为完全变量，从缺失的分布来说又分为完 全随机缺失，随机缺失和完全非随机缺失：

完全随机缺失：指的是数据的缺失是完全随机的；
随机缺失：指的是数据的缺失不是完全随机的，和完全变量关；
完全不随机缺失：指的是数据的缺失与不完全变量自身的取值相关； 缺失值会使得系统丢失了大量的有用信息，系统所表现出来的不确定性更加显著，系统中蕴含的确定性成分更难把握，包含空值的不完全变量会使得挖掘过程陷入混乱。
下面来先讲一下缺失值的处理：

1.1 删除法，主要有简单删除法
1.1.1 简单删除法适合于缺失值样本比较少的情况下，如果有过多的缺失值，则不适合使用该方法，因为该方法是用减少历史数据的方法来换取数据的完备性，这样会造成资源的极大浪费，因为其丢弃了大量隐藏在这些对象上的信息，在样本数量本来就很少的数据集中删除少量对象将严重影响数据集的客观性和结果的正确性；
1.2 数据补齐，主要有人工填写、特殊值填充、平均值填充、热卡填充、K-means填充，使用所有可能的值填充、组合完整化方法，回归法，期望值最大化方法，多重填补以及C4.5方法；
1.2.1 人工填写，用户自己对数据最为了解，因此这个方法产生的偏差是最小的，但是如果有大规模的缺失值时，这个方法是非常耗时耗力的；
1.2.2 特殊值填充，将空值作为一种特殊值来处理，但是有可能造成严重的数据偏离，一般不推荐使用；
1.2.3 平均值填充，如果是数值型特征，则是使用平均值来填充，如果是类别型特征，则是使用众数来填充，另一种相似的方法是条件平均值填充，这个并不是直接使用所有对象来计算平均值或者众数，而是使用与该样本具有相同决策属性的对象中去求解平均值或者众数；
1.2.4 热卡填充，或者说就叫做近补齐，对于一个包含空值的对象，热卡填充法在完整数据集中找到一个与它最相似的对象，用这个值来填充；
1.2.5 k-means利用欧式距离或者相关性分析来确定距离最近的K个样本，将这K个值的加权平均值来估计该样本的缺失数据；
1.2.6 组合完整化方法，用空缺属性值的所有可能的属性取值来试，并从最终属性的约间结果中选择一个最好的属性值；
1.2.7 使用所有可能的值填充，使用所有可能的属性值来填充，能够得到很好的效果；
1.2.8 回归，基于完整的数据集，建立回归方程。对于包含空值的对象，将已知数据集带入回归方程来估计预测值，并以此预测值来进行填充，但是当变量不是线性相关时则会导致偏差的估计；
1.2.9 期望值最大化方法，在不完全数据情况下计算极大似然估计和后验分布的迭代算法；
1.2.10 多重填补
1.2.11 C4.5
1.3 不处理，直接在包含空值的数据集上进行处理比如贝叶斯网络和人工神经网络

## 不平衡数据处理

所谓的不平衡数据集指的是数据集各个类别的样本量极不均衡。以二分类问题为例，假设正类的样本数量远大于负类的样本数量，通常情况下通常情况下把多数类样本的比例接近100:1这种情况下的数据称为不平衡数据。不平衡数据的学习即需要在分布不均匀的数据集中学习到有用的信息。

不平衡数据集的处理方法主要分为两个方面：

1、从数据的角度出发，主要方法为采样，分为欠采样和过采样以及对应的一些改进方法。

2、从算法的角度出发，考虑不同误分类情况代价的差异性对算法进行优化，主要是基于代价敏感学习算法(Cost-Sensitive Learning)，代表的算法有adacost；

另外可以将不平衡数据集的问题考虑为一分类（One Class Learning）或者异常检测（Novelty Detection）问题，代表的算法有One-class SVM。

## 过拟合和欠拟合

如何处理欠拟合
欠拟合是由于学习不足，可以考虑添加特征，从数据中挖掘出更多的特征，有时候还需要对特征进行变换，使用组合特征和高次特征。

模型简单也会导致欠拟合，例如线性模型只能拟合一次函数的数据。尝试使用更高级的模型有助于解决欠拟合，如使用SVM，神经网络等。

正则化参数是用来防止过拟合的，出现欠拟合的情况就要考虑减少正则化参数。

如何处理过拟合
过拟合是由于学习的太彻底，这可能是由于训练数据量太少的缘故。可以增大数据的训练量，训练数据要足够大才能使得数据中的特征被模型学习到。还需要清洗数据，尽量减少数据中的噪声，以防止这些噪声被模型学习到。

正则化方法也常用来处理过拟合，正则化包括L1正则化和L2正则化，正则项通常是一个范数。L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”，通常可以用来做特征选择，在嵌入式特征选择使用的就是L1范数。L2范数是指向量各元素的平方和然后求平方根，通常的结果会使权重参数变小，使得模型的复杂度变低，符合奥卡姆剃刀原则，以防止过拟合。

在神经网络中经常使用Dropout方法，或者称之为随机失活，每次训练的时候随机去掉一部分隐藏层的神经元，可以理解为每个神经元随机参与，相当于多个模型集成。

提前终止（early stoppping）也是神经网络常使用的方法，可以防止模型复杂度过于增加，从而防止过拟合。采用交叉验证提前终止，当交叉验证错误率最小时认为泛化性能最好，这时即使训练集错误率仍然下降，也终止训练。

逐层归一化（batch normalization），给每层的输出做归一化（相当于加了一个线性变换层），这样使得下一层的输入相当于高斯分布（正态分布），这个方法相当于下一层的权重参数训练时避免了其输入以偏概全, 因而泛化效果也比较好。

不过，数据在训练过程中始终是最重要的，有时候往往拥有更多的数据胜过一个好的模型，这要求得到更多独立同分布的数据来进行训练。
